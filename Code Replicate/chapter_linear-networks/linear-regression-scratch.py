import matplotlib.pyplot as plt
import torch
import random


# ç”Ÿæˆy=Xw+b+å™ªå£°
def synthetic_data(w, b, num_examples):
    X = torch.normal(mean=0, std=1, size=(num_examples, len(w)))
    # ç”Ÿæˆä¸€ä¸ªå¤§å°ä¸º num_examples(æ ·æœ¬å®¹é‡:æ ·æœ¬ä¸ªæ•°)*len(w)(ç‰¹å¾ä¸ªæ•°,ä¸æƒé‡ä¸ªæ•°ç›¸åŒ)
    # ä¾‹å¦‚æ¯ä¸ªæ ·æœ¬æœ‰ä¸¤ä¸ªç‰¹å¾ï¼Œä¸€å…±æœ‰ä¸€åƒä¸ªæ ·æœ¬ï¼Œåˆ™æ„æˆçŸ©é˜µä¸º ğ—âˆˆâ„1000Ã—2 wâˆˆâ„2Ã—1
    # X*wâˆˆâ„1000Ã—1ä¸º1000ä¸ªé¢„æµ‹è¾“å‡º
    y = torch.matmul(X, w) + b  # matrix multiplication çŸ©é˜µç§¯ï¼Œéå‰ä¹˜
    # add noise
    y += torch.normal(mean=0, std=0.01, size=y.shape)
    return X, y.reshape((-1, 1))
    # X.shape torch.Size([1000, 2])  äºŒç»´å¼ é‡/çŸ©é˜µ
    # y.shape torch.Size([1000, 1])  ä¸€ç»´å¼ é‡/å‘é‡
    # åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨ PyTorch ç­‰æ·±åº¦å­¦ä¹ æ¡†æ¶æ—¶ï¼Œé€šå¸¸å¯¹æ ‡ç­¾è¿›è¡ŒäºŒç»´å¼ é‡çš„ reshape æ˜¯ä¸ºäº†ä¿æŒä¸€è‡´æ€§ã€‚
    # è¿™å¯ä»¥å¸®åŠ©é¿å…åœ¨è®¡ç®—ä¸­å¼•å…¥ä¸å¿…è¦çš„å¤æ‚æ€§ï¼Œå¹¶ä½¿ä»£ç æ›´åŠ æ¸…æ™°ã€‚ä¸ºäº†ä¿æŒä¸€è‡´æ€§ï¼Œå¯ä»¥å°†æ ‡ç­¾ y reshapeæˆä¸€ä¸ªå½¢çŠ¶ä¸º 1000Ã—1çš„åˆ—å‘é‡
    # è¿™æ ·å®ƒçš„å½¢çŠ¶å°±ä¸æ¨¡å‹è¾“å‡ºçš„å½¢çŠ¶ä¸€è‡´äº†ã€‚è¿™åœ¨è®¸å¤šæ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­æ˜¯ä¸€ä¸ªé€šç”¨çš„åšæ³•ï¼Œå®ƒæœ‰åŠ©äºç®€åŒ–ä»£ç ï¼Œå¹¶ç¡®ä¿åœ¨å¤„ç†æ¨¡å‹è¾“å‡ºå’Œæ ‡ç­¾æ—¶æ²¡æœ‰ç»´åº¦ä¸åŒ¹é…çš„é—®é¢˜


# torch.Tensor() å’Œ torch.tensor()çš„åŒºåˆ«
# torch.Tensor() æ˜¯ä¸€ä¸ªæ„é€ å‡½æ•°ï¼Œç”¨äºåˆ›å»ºå¼ é‡ï¼Œå¯ä»¥ç”¨äºåˆ›å»ºç©ºå¼ é‡æˆ–ä»ç°æœ‰å¼ é‡åˆ›å»ºæ–°å¼ é‡ã€‚
# torch.tensor() æ˜¯ä¸€ä¸ªå·¥å‚å‡½æ•°ï¼Œç”¨äºä»ç»™å®šæ•°æ®åˆ›å»ºæ–°çš„å¼ é‡ï¼Œå¹¶ä¸”èƒ½å¤Ÿç›´æ¥ä» Python åˆ—è¡¨ã€NumPy æ•°ç»„ç­‰æ•°æ®ç±»å‹ä¸­åˆ›å»ºã€‚
true_w = torch.Tensor([2, -3.4])
true_b = torch.Tensor([4.2])
features, labels = synthetic_data(true_w, true_b, 1000)

plt.figure(figsize=(4,3))
# featuresæ˜¯ä¸€ä¸ªmxnçŸ©é˜µ(m:æ ·æœ¬å®¹é‡ï¼Œn:ç‰¹å¾) labelsæ˜¯å¯¹åº”ç›®æ ‡æ ‡ç­¾ï¼Œ 1ä¸ºæ•£ç‚¹å¤§å°
# ç¬¬ä¸€ä¸ªç‰¹å¾å’Œå’Œlabelsçš„æ•£ç‚¹å›¾
# plt.scatter(features[:, 0], labels, 1)
# ç¬¬äºŒä¸ªç‰¹å¾å’Œlabelsçš„æ•£ç‚¹å›¾
plt.scatter(features[:, 1], labels, 1)
plt.show()


# ä»åŸå§‹æ•°æ®é›†ä¸­è·å–å°æ‰¹é‡æ•°æ®
def data_iter(batch_size, features, labels):
    # len()ä¼šé€‰å–ç¬¬ä¸€ç»´çš„é•¿åº¦ = 1000
    nums_features = len(features)
    # åˆ©ç”¨è¿™ä¸ªé•¿åº¦ç”Ÿæˆä¸€ä¸ªåˆ—è¡¨åŒ…å«åŒç­‰æ•°é‡çš„å…ƒç´ ä¸º0-nums_features, å¹¶æ‰“ä¹±
    indices = list(range(nums_features))
    random.shuffle(indices)
    # ä»0å¼€å§‹ç›´åˆ°æœ€åä¸€ä¸ªå…ƒç´ ,æ¯æ¬¡æŠ½å–ä¸€ä¸ªbatch_size
    for i in range(0, nums_features, batch_size):
        # å®šä¹‰ä¸€ä¸ªå¼ é‡,æ¯æ¬¡ä»éšæœºçš„ä¸‹æ ‡åˆ—è¡¨ä¸­ä»iå…ƒç´ å¼€å§‹æŠ½å–ä¸€ä¸ªbatch,é™¤éæœ€åä¸€ä¸ªbatch,æŠ½åˆ°æœ€åä¸€ä¸ªå…ƒç´ ä½ç½®(å¦‚æœå‰©ä½™å…ƒç´ å°‘äºbatch_size)
        batch_indicices = torch.tensor(indices[i : min(i+batch_size, nums_features)])
        # ä½¿ç”¨ yield å…³é”®å­—ç”Ÿæˆä¸€ä¸ªæ‰¹é‡çš„ç‰¹å¾å’Œå¯¹åº”çš„æ ‡ç­¾ï¼Œå¹¶è¿”å›ç»™è°ƒç”¨æ–¹ã€‚
        # yield çš„ä½œç”¨æ˜¯å°†å‡½æ•°å˜æˆä¸€ä¸ªç”Ÿæˆå™¨ï¼Œå…è®¸ä½ åœ¨è¿­ä»£ä¸­é€æ­¥äº§ç”Ÿå€¼ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰å€¼ã€‚
        # å®šä¹‰ä¸€ä¸ªå¾ªç¯ï¼Œå¯ä»¥å¾ªç¯æ¥å—ç”Ÿæˆå™¨ä¸­çš„è¿”å›å€¼ï¼Œå¾ªç¯æ¡ä»¶æ¯æ‰§è¡Œä¸€æ¬¡ï¼Œå°±ä¼šæŠ½å–ä¸€æ¬¡æ•°æ®
        # ç›¸å½“äºä¸€ä¸ªdataloader
        # !!ç›´æ¥ä½¿ç”¨åˆ—è¡¨ä¹Ÿå¯ä»¥
        yield features[batch_indicices], labels[batch_indicices]
        # å¦‚æœbatch_size=10å°±ä¼šè¿”å›10ç»„æ•°æ®ï¼Œå¹¶ä¸”ç­‰å¾…ä¸‹ä¸€æ¬¡æŠ½å–(ä¹Ÿå°±æ˜¯ä¸‹ä¸€æ¬¡æ‰§è¡Œå¾ªç¯æ¡ä»¶)
        """
        batch_size = 10
        batch_index = 0
        for X, y in data_iter(batch_size, features, labels): æŠ½å–æ•°æ®ï¼Œè¿™å°±æ˜¯yieldå’Œreturnçš„åŒºåˆ«
            # returnä¼šä¸€æ¬¡æ€§è¿”å›æ‰€æœ‰æ•°æ®
            # yieldä¼šå°†å‡½æ•°è½¬å˜ä¸ºä¸€ä¸ªç”Ÿæˆå™¨ï¼Œå…è®¸åœ¨è¿­ä»£ä¸­é€æ­¥äº§ç”Ÿå€¼ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§äº§ç”Ÿå€¼
            print(X, '\n', y)
            print(batch_index)
            batch_index += 1
        """


batch_size = 10
# åˆå§‹åŒ–æƒé‡ä¸åç½®ï¼Œä¸¤è€…éƒ½æ˜¯æˆ‘ä»¬è¦æ›´æ–°çš„å‚æ•°ï¼Œæ‰€ä»¥è®¾ç½®ä¿ç•™æ¢¯åº¦
# æƒé‡ä¸€èˆ¬åˆå§‹åŒ–ä¸ºmean=0, std=0.01çš„æ­£æ€åˆ†å¸ƒ
# åæ‰§ä¸€èˆ¬è®¾ç½®ä¸º0
w = torch.normal(mean=0, std=0.01, size=(2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)


def linear_reg(X, w, b):
    # æ„å»ºçº¿æ€§åŒ–æ¨¡å‹ä¸åˆ›å»ºæ•°æ®é›†çš„æ¨¡å‹ç›¸åŒï¼Œåªæ˜¯æ•°æ®ä¸åŒ
    # (1000,2)*(2,1) => (1000,1)
    return torch.matmul(X, w) + b


def squared_loss(pred, true):
    # predæ˜¯ä»æ¨¡å‹ä¸­ç”Ÿæˆçš„ ç»´åº¦ä¸º(1000,1)
    # trueåŒæ ·ä¹Ÿæ˜¯æ¨¡å‹ç”Ÿæˆçš„ ç»´åº¦ä¸º(1000,1)
    # ä¸ªäººäººä¸ºä¸éœ€è¦æ·»åŠ reshape, ä»£ç ç¡®å®å¯ä»¥è·‘é€š
    # return (pred - true.reshape(pred.shape)) ** 2 / 2
    return (pred - true)**2 / 2


# ä¼˜åŒ–å™¨, ä¼˜åŒ–å™¨ä¸­çš„æ¢¯åº¦æ›´æ–°ä¸ºæ•°å­—è¿ç®—ï¼Œä¸éœ€è¦è®¡ç®—æ¢¯åº¦
# parametersä¼ å…¥çš„æ˜¯æ‰€æœ‰éœ€è¦æ›´æ–°çš„å‚æ•°
def sgd(parameters, learning_rate, batch_length):
    with torch.no_grad():
        for parameter in parameters:
            # è¿™é‡Œçš„batchsize å…¶å®æœ‰é—®é¢˜
            # æŠ½å–çš„indiciceså¹¶ä¸ä¸€å®šæ˜¯batch_size
            # æŸ¥çœ‹æ‰“å°ç»“æœä¼šå‘ç°æœ€åä¸€ä¸ªbatchåªæœ‰8ä¸ªæ ·æœ¬
            # ä¸ºä»€ä¹ˆè¦é™¤ä»¥æ ·æœ¬å®¹é‡ï¼Ÿå› ä¸ºæœ€åæ˜¯å¯¹loss.sum()æ±‚æ¢¯åº¦
            parameter -= learning_rate * parameter.grad / batch_length
            # æ¸…ç©ºæ¢¯åº¦
            parameter.grad.zero_()


lr = 0.001
num_epoch = 3
net = linear_reg
loss = squared_loss

for epoch in range(num_epoch):
    # æŠ½å–ä¸€ç»„minibatchçš„æ ·æœ¬(è¾“å…¥ä¸è¾“å‡ºå¯¹åº”å…³ç³»)
    for X, y in data_iter(batch_size=batch_size, features=features, labels=labels):
        # è®¡ç®—æŸå¤±, y_hatæ˜¯æ•°æ®ç»è¿‡å½“å‰ç½‘ç»œå¾—åˆ°çš„è¾“å‡º
        l = loss(net(X, w, b), y)
        batch_length = len(l)
        # å¯¹lossæ±‚å’Œ, å¹¶è®¡ç®—bp
        l.sum().backward()
        # å¯¹ç›®æ ‡å˜é‡æ›´æ–°
        sgd([w ,b], lr, batch_length)
    with torch.no_grad():
        # train_l.shape = torch.Size([1000, 1])
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')


print(f'wçš„ä¼°è®¡è¯¯å·®: {true_w - w.reshape(true_w.shape)}')
print(f'bçš„ä¼°è®¡è¯¯å·®: {true_b - b}')

